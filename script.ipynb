{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb4fa2cc-4bcb-4b18-9693-372e6ff9b6b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c243723-8f66-4b79-ac52-8032c7f95d22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importando bibliotecas para manipulação de dados com PySpark\n",
    "from pyspark.sql.functions import (\n",
    "    regexp_extract, # Função para extrair uma substring usando expressão regular\n",
    "    col,            # Função para referenciar uma coluna para operações\n",
    "    date_format,    # Função para formatar uma coluna de data para um formato específico\n",
    "    to_date,        # Função para converter uma string para o tipo de dado de data\n",
    "    expr,           # Função para usar expressões SQL diretamente no código Spark\n",
    "    avg,            # Função para calcular a média de uma coluna\n",
    "    max,            # Função para calcular o valor máximo de uma coluna\n",
    "    min,            # Função para calcular o valor mínimo de uma coluna\n",
    "    sum,            # Função para calcular a soma dos valores de uma coluna\n",
    "    when            # Função para aplicar condições (semelhante ao IF)\n",
    ")\n",
    "\n",
    "# Importando biblioteca para definição de tipos para schema do DataFrame\n",
    "from pyspark.sql.types import IntegerType  \n",
    "\n",
    "# Importando bibliotecas para operações Delta Tables (transações ACID no Spark)\n",
    "from delta.tables import *\n",
    "\n",
    "# Importando biblioteca para manipulação de datas\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f572984-f856-45b1-80e1-8b95478b1316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Configuração do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f2d7f9-34b3-4ba6-a9fd-95b898ea34b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inicializando a SparkSession\n",
    "spark = SparkSession.builder.appName(\"LogAnalysis\").getOrCreate()\n",
    "\n",
    "# Parâmetros do Blob Storage\n",
    "# Nome da conta de armazenamento no Azure\n",
    "storage_account_name = \"cloudcasefirst\"\n",
    "# Nome do contêiner onde os dados estão armazenados\n",
    "container_name = \"accesslog\"\n",
    "# Caminho WABS (Windows Azure Storage Blob Service)\n",
    "blob_storage_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\"\n",
    "#Token SAS (Shared Access Signature) temporário para autenticação no Blob Storage\n",
    "sas_token = \"sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-04-10T21:15:56Z&st=2025-01-23T13:15:56Z&spr=https&sig=uKPFUqwDYzqfz4BGe2H7Cn1nq4mLmIna7OcCUKYK%2F0s%3D\"\n",
    "\n",
    "# Configuração do Blob Storage com SAS\n",
    "# Associando o token SAS ao contêiner para permitir o acesso aos dados\n",
    "spark.conf.set(f\"fs.azure.sas.{container_name}.{storage_account_name}.blob.core.windows.net\", sas_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f459002f-a679-45f2-8aad-79cf6f7f2396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Variáveis Paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d133752-a95f-4a45-8448-be2f7ae38d31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Variáveis de Ambiente\n",
    "full_load = \"full_load/\"\n",
    "access_log = \"access_log.txt\"\n",
    "trigger = \"trigger/trigger\"\n",
    "access_log_delta = \"delta_table_accessLog\"\n",
    "log_delta = \"log_delta\"\n",
    "\n",
    "# Definição de Paths (Caminhos de Acesso)\n",
    "file_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{access_log}\"\n",
    "log_delta_path = f\"{blob_storage_path}{log_delta}\"\n",
    "full_load_path = f\"{blob_storage_path}{full_load}\"\n",
    "trigger_path = f\"{blob_storage_path}{trigger}\"\n",
    "delta_table_acessLog_path = f\"{blob_storage_path}{access_log_delta}\"\n",
    "\n",
    "# Lista Global para Armazenar Logs Processados\n",
    "processing_log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c26b687c-b64a-41c1-b5be-83aa74c41332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd8a0aa7-99c2-4c16-84e7-3acc63caefc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registrar Logs: Adiciona eventos à lista de rastreamento da pipeline.\n",
    "def log_event(step, status, message=\"\", record_count=None):\n",
    "    \"\"\"\n",
    "    Registra um evento no log de processamento.\n",
    "\n",
    "    Parâmetros:\n",
    "    - step (str): Nome ou descrição da etapa do pipeline sendo registrada.\n",
    "    - status (str): Status da execução na etapa (por exemplo, \"Iniciado\", \"Sucesso\", \"Erro\").\n",
    "    - message (str, opcional): Mensagem adicional com detalhes do evento. Valor padrão é uma string vazia.\n",
    "    - record_count (int, opcional): Número de registros processados na etapa. Valor padrão é None.\n",
    "\n",
    "    Retorna:\n",
    "    A função adiciona o evento registrado à lista global 'processing_log'.\n",
    "    \"\"\"\n",
    "    event = Row(\n",
    "        Timestamp=str(datetime.now()),  # Data e horário do evento\n",
    "        PipelineStep=step,  # Etapa da pipeline associada ao evento\n",
    "        Status=status,  # Status da execução na etapa\n",
    "        Message=message,  # Mensagem adicional informativa (se fornecida)\n",
    "        RecordCount=record_count  # Quantidade de registros processados (se fornecida)\n",
    "    )\n",
    "    processing_log.append(event)  # Adiciona o evento registrado à lista global de logs\n",
    "\n",
    "# Ler o Arquivo de Log: Lê arquivo de log e retorna um DataFrame.\n",
    "def read_logs(file_path):\n",
    "    \"\"\"\n",
    "    Lê um arquivo de log e retorna um DataFrame com seu conteúdo.\n",
    "\n",
    "    Parâmetros:\n",
    "    - file_path (str): Caminho completo do arquivo de log a ser lido.\n",
    "\n",
    "    Retorna:\n",
    "    - logs_df (DataFrame): DataFrame contendo o conteúdo do arquivo de log.\n",
    "    \"\"\"\n",
    "    step_name = \"Read Access Logs\"  # Nome da etapa do pipeline para identificar no log de execução\n",
    "    try:\n",
    "        log_event(step_name, \"Inicio\", f\"Lendo arquivo de log de {file_path}\")  # Registrando o início da leitura do arquivo de log\n",
    "        logs_df = spark.read.text(file_path)  # Lendo o arquivo de log e criando um DataFrame\n",
    "        log_event(step_name, \"Sucesso\", record_count=logs_df.count())  # Registrando o sucesso da leitura, incluindo a quantidade de registros lidos\n",
    "        return logs_df  # Retornando o DataFrame com os dados lidos\n",
    "    except Exception as e:\n",
    "        log_event(step_name, \"Erro\", message=str(e))  # Registrando o erro caso ocorra e levantando novamente a exceção\n",
    "        raise\n",
    "\n",
    "# Criar camada Silver: Extrair campos dos Logs com expressões regulares.\n",
    "def bronze_to_silver(logs_df):\n",
    "    \"\"\"\n",
    "    Extrai campos dos logs usando expressões regulares e os transforma para a camada Silver.\n",
    "\n",
    "    Parâmetros:\n",
    "    - logs_df (DataFrame): DataFrame contendo os logs em formato bruto.\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame: DataFrame contendo os campos extraídos e transformados para a camada Silver.\n",
    "    \"\"\"\n",
    "    step_name = \"Bronze to Silver\"  # Nome da etapa para rastrear o log de execução\n",
    "    try:\n",
    "        log_event(step_name, \"Inicio\", \"Transformando logs para camada Silver\")  # Registrando o início da transformação\n",
    "        ip_regex = r'^(\\S+)'  # Expressão regular para extrair o IP\n",
    "        timestamp_regex = r'\\[(.*?)\\]'  # Expressão regular para extrair o timestamp\n",
    "        method_uri_protocol_regex = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'  # Expressão regular para extrair método, URI e protocolo\n",
    "        status_regex = r'\\s(\\d{3})\\s'  # Expressão regular para extrair o código de status\n",
    "        bytes_regex = r'\\s(\\d+)$'  # Expressão regular para extrair o número de bytes\n",
    "        log_event(step_name, \"Sucesso\")  # Registrando o sucesso da transformação\n",
    "        return logs_df.select(\n",
    "            regexp_extract('value', ip_regex, 1).alias('IP'),  # Extraindo o IP\n",
    "            regexp_extract('value', timestamp_regex, 1).alias('Timestamp'),  # Extraindo o timestamp\n",
    "            regexp_extract('value', method_uri_protocol_regex, 1).alias('Method'),  # Extraindo o método\n",
    "            regexp_extract('value', method_uri_protocol_regex, 2).alias('URI'),  # Extraindo a URI\n",
    "            regexp_extract('value', method_uri_protocol_regex, 3).alias('Protocol'),  # Extraindo o protocolo\n",
    "            regexp_extract('value', status_regex, 1).alias('Status'),  # Extraindo o status\n",
    "            regexp_extract('value', bytes_regex, 1).cast(IntegerType()).alias('Bytes')  # Extraindo e convertendo os bytes\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log_event(step_name, \"Erro\", message=str(e))  # Registrando o erro caso ocorra\n",
    "        raise  # Levantando novamente a exceção\n",
    "\n",
    "# Criar camada Gold: Aadicionar e formatar colunas.\n",
    "def silver_to_gold(logs_parsed_df):\n",
    "    \"\"\"\n",
    "    Adiciona e formata as colunas de data, dia da semana e categoria de status, transformando os dados da camada Silver para a camada Gold.\n",
    "\n",
    "    Parâmetros:\n",
    "    - logs_parsed_df (DataFrame): DataFrame contendo os logs da camada Silver.\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame: DataFrame com colunas adicionais e transformadas para a camada Gold.\n",
    "    \"\"\"\n",
    "    step_name = \"Silver to Gold\"  # Nome da etapa para rastrear o log de execução\n",
    "    try:\n",
    "        log_event(step_name, \"Inicio\", \"Transformando logs para camada Gold\")  # Registrando o início da transformação\n",
    "        logs_parsed_df = logs_parsed_df.withColumn(\n",
    "            'Date',  # Adicionando a coluna 'Date'\n",
    "            functions.regexp_extract('Timestamp', r'^(.*?)\\:', 1)  # Extraindo a data do timestamp\n",
    "        ).withColumn(\n",
    "            'Date', functions.to_date('Date', 'dd/MMM/yyyy')  # Convertendo para formato de data\n",
    "        ).withColumn(\n",
    "            'Day_of_Week', functions.expr('EXTRACT(DAYOFWEEK FROM Date)')  # Extraindo o dia da semana\n",
    "        ).withColumn(\n",
    "            'Day_of_Week_Name',  # Adicionando o nome do dia da semana\n",
    "            when(col('Day_of_Week') == 1, \"Domingo\")  # Mapeando os valores do dia da semana\n",
    "            .when(col('Day_of_Week') == 2, \"Segunda\")\n",
    "            .when(col('Day_of_Week') == 3, \"Terça\")\n",
    "            .when(col('Day_of_Week') == 4, \"Quarta\")\n",
    "            .when(col('Day_of_Week') == 5, \"Quinta\")\n",
    "            .when(col('Day_of_Week') == 6, \"Sexta\")\n",
    "            .when(col('Day_of_Week') == 7, \"Sábado\")\n",
    "            .otherwise(\"desconhecido\")  # Caso o dia da semana não seja reconhecido\n",
    "        ).withColumn(\n",
    "            'Categoria_Status',  # Adicionando a coluna 'Categoria_Status'\n",
    "            functions.when((functions.col('Status') >= 200) & (functions.col('Status') < 300), \"Sucesso\")  # Mapeando status de sucesso\n",
    "            .when((functions.col('Status') >= 300) & (functions.col('Status') < 400), \"Redirecionamento\")  # Mapeando status de redirecionamento\n",
    "            .when((functions.col('Status') >= 400) & (functions.col('Status') < 500), \"Erro do Cliente\")  # Mapeando erro do cliente\n",
    "            .when((functions.col('Status') >= 500) & (functions.col('Status') < 600), \"Erro do Servidor\")  # Mapeando erro do servidor\n",
    "            .otherwise(\"Desconhecido\")  # Caso o status não se enquadre em nenhum grupo\n",
    "        ).withColumn(\n",
    "            'Endpoint',  # Adicionando a coluna 'Endpoint'\n",
    "            when(col('URI') == \"/\", \"home-page\")  # Se a URI for '/', o endpoint é 'home-page'\n",
    "            .when(col('URI').rlike(r'\\.\\w+$'), \"\")  # Se a URI terminar com uma extensão, o endpoint fica vazio\n",
    "            .otherwise(regexp_extract(col('URI'), r'^/([^/]+)(?:/|$)', 1))  # Extraindo o primeiro segmento da URI\n",
    "        )\n",
    "        log_event(step_name, \"Sucesso\")  # Registrando o sucesso da transformação\n",
    "        return logs_parsed_df  # Retornando o DataFrame com as novas colunas\n",
    "    except Exception as e:\n",
    "        log_event(step_name, \"Erro\", message=str(e))  # Registrando qualquer erro que ocorra\n",
    "        raise  # Levantando novamente a exceção\n",
    "\n",
    "# Salvar dados no Blob Storage: Salva dados processados em formato Parquet.\n",
    "def save_to_blob(logs_parsed_df, path):\n",
    "    \"\"\"\n",
    "    Salva os dados processados em formato Parquet no Blob Storage.\n",
    "\n",
    "    Parâmetros:\n",
    "    - logs_parsed_df (DataFrame): DataFrame contendo os dados processados a serem salvos.\n",
    "    - path (str): Caminho de destino no Blob Storage onde os dados serão salvos.\n",
    "\n",
    "    Retorna:\n",
    "    - Um arquivo Parquet a ser consumido pelo Power Bi.\n",
    "    \"\"\"\n",
    "    step_name = \"Save to Blob\"  # Nome da etapa para rastrear o log de execução\n",
    "    try:\n",
    "        log_event(step_name, \"Inicio\", f\"Salvando dados processados em {path}\")  # Registrando o início da operação\n",
    "        logs_parsed_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(path)  # Salvando os dados em formato Parquet com sobrescrita\n",
    "        log_event(step_name, \"Sucesso\")  # Registrando o sucesso da operação\n",
    "    except Exception as e:\n",
    "        log_event(step_name, \"Erro\", message=str(e))  # Registrando qualquer erro que ocorra\n",
    "        raise  # Levantando novamente a exceção\n",
    "\n",
    "# Ativar o Trigger no Power Automate: Aciona trigger no Power Automate com DBUtils.\n",
    "def trigger():\n",
    "    \"\"\"\n",
    "    Manipula um trigger no Power Automate, escrevendo um conteúdo vazio no caminho especificado para iniciar o processo.\n",
    "\n",
    "    Parâmetros:\n",
    "    Nenhum.\n",
    "\n",
    "    Retorna:\n",
    "    Nenhum.\n",
    "    \"\"\"\n",
    "    step_name = \"Trigger\"  # Nome da etapa para rastrear o log de execução\n",
    "    try:\n",
    "        log_event(step_name, \"Inicio\", f\"Acionando o Trigger\")  # Registrando o início da operação\n",
    "        content = \"\"  # Conteúdo vazio para ativar o trigger\n",
    "        dbutils.fs.put(trigger_path, content, overwrite=True)  # Escrevendo o conteúdo no caminho\n",
    "        log_event(step_name, \"Sucesso\")  # Registrando o sucesso da operação\n",
    "    except Exception as e:\n",
    "        log_event(step_name, \"Erro\", message=str(e))  # Registrando erro no log\n",
    "        raise  # Levantando novamente a exceção\n",
    "\n",
    "# Salvar o Access Log estruturado em Delta: Salva DataFrame de logs como tabela Delta.\n",
    "def save_as_delta_table(df, table_name, delta_path):\n",
    "    \"\"\"\n",
    "    Salva o DataFrame como uma tabela Delta no caminho especificado para otimizar as análises.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df (DataFrame): O DataFrame de logs a ser salvo.\n",
    "    - table_name (str): O nome da tabela Delta a ser criada ou sobrescrita.\n",
    "    - delta_path (str): O caminho onde os dados Delta serão armazenados.\n",
    "\n",
    "    Retorna:\n",
    "    Nenhum.\n",
    "    \"\"\"\n",
    "    step_name = \"Full Load to Delta\"  # Nome da etapa para rastrear o log de execução\n",
    "    try:\n",
    "        log_event(step_name, \"Inicio\", f\"Salvando DataFrame como tabela Delta: {table_name}\")  # Registrando o início da operação\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")  # Removendo a tabela existente, caso exista\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)  # Salvando o DataFrame no formato Delta\n",
    "        spark.sql(f\"CREATE TABLE {table_name} USING DELTA LOCATION '{delta_path}'\")  # Criando a tabela Delta\n",
    "        log_event(step_name, \"Sucesso\", record_count=df.count())  # Registrando o sucesso da operação com a contagem de registros\n",
    "    except Exception as e:\n",
    "        log_event(step_name, \"Erro\", message=str(e))  # Registrando erro no log\n",
    "        raise  # Levantando novamente a exceção\n",
    "\n",
    "# Contar as 10 maiores origens de acesso.\n",
    "def top_ip_count(logs_df):\n",
    "    \"\"\"\n",
    "    Conta as 10 origens de acesso (IPs) mais frequentes nos logs.\n",
    "\n",
    "    Parâmetros:\n",
    "    - logs_df (DataFrame): DataFrame contendo os logs com a coluna \"IP\".\n",
    "\n",
    "    Retorna:\n",
    "    - top_ips (DataFrame): DataFrame com as 10 origens de acesso mais frequentes, incluindo a contagem.\n",
    "    \"\"\"\n",
    "    step_name = \"Top 10 - IP Count\"  # Nome da etapa para rastrear o log de execução\n",
    "    try:\n",
    "        log_event(step_name, \"Inicio\", \"Contando as 10 maiores origens de acesso\")  # Registrando o início da operação\n",
    "        top_ips = logs_df.groupBy(\"IP\").count().orderBy(col(\"count\").desc()).limit(10)  # Calculando as 10 origens de acesso mais frequentes\n",
    "        top_ips.show(truncate=False)  # Exibindo as 10 maiores origens de acesso\n",
    "        log_event(step_name, \"Sucesso\", record_count=top_ips.count())  # Registrando o sucesso da operação com a contagem de registros\n",
    "    except Exception as e:\n",
    "        log_event(step_name, \"Erro\", message=str(e))  # Registrando erro no log\n",
    "        raise  # Levantando novamente a exceção\n",
    "\n",
    "# Listar os 6 endpoints mais acessados. \n",
    "def top_endpoints_count(logs_df):\n",
    "    \"\"\"\n",
    "    Conta os 6 endpoints mais acessados, desconsiderando registros de arquivos (Endpoint vazio).\n",
    "\n",
    "    Parâmetros:\n",
    "    - logs_df (DataFrame): DataFrame contendo os logs com a coluna \"Endpoint\".\n",
    "\n",
    "    Retorna:\n",
    "    - top_endpoints (DataFrame): DataFrame com os 6 endpoints mais acessados, incluindo a contagem de acessos.\n",
    "    \"\"\"\n",
    "    step_name = \"Top 6 - Endpoints Count\"  # Nome da etapa para rastrear o log de execução\n",
    "    try:\n",
    "        log_event(step_name, \"Inicio\", \"Contando os 6 endpoints mais acessados\")  # Registrando o início da operação\n",
    "        non_empty_endpoints = logs_df.filter(col(\"Endpoint\") != \"\")  # Filtrando os logs para desconsiderar os registros com Endpoint vazio\n",
    "        top_endpoints = non_empty_endpoints.groupBy(\"Endpoint\").count().orderBy(col(\"count\").desc()).limit(6)  # Calculando os 6 endpoints mais acessados\n",
    "        top_endpoints.show(truncate=False)  # Exibindo os 6 endpoints mais acessados\n",
    "        log_event(step_name, \"Sucesso\", record_count=top_endpoints.count())  # Registrando o sucesso da operação com a contagem de registros\n",
    "    except Exception as e:\n",
    "        log_event(step_name, \"Erro\", message=str(e))  # Registrando erro no log\n",
    "        raise  # Levantando novamente a exceção\n",
    "\n",
    "# Contar a quantidade de IPs distintos.\n",
    "def distinct_ips(logs_df):\n",
    "    \"\"\"\n",
    "    Conta a quantidade de IPs distintos nos logs.\n",
    "\n",
    "    Parâmetros:\n",
    "    - logs_df (DataFrame): DataFrame contendo os logs com a coluna \"IP\".\n",
    "\n",
    "    Retorna:\n",
    "    - distinct_ips_count (int): Quantidade de IPs distintos encontrados nos logs.\n",
    "    \"\"\"\n",
    "    step_name = \"Distinct IPs\"  # Nome da etapa para rastrear o log de execução\n",
    "    try:\n",
    "        log_event(step_name, \"Inicio\", \"Contando a quantidade de IPs distintos\")  # Registrando o início da operação\n",
    "        distinct_ips_count = logs_df.select(\"IP\").distinct().count()  # Contando os IPs distintos\n",
    "        print(f\"Quantidade de IPs distintos: {distinct_ips_count}\")  # Exibindo o resultado\n",
    "        log_event(step_name, \"Sucesso\", record_count=distinct_ips_count)  # Registrando o sucesso da operação com a contagem\n",
    "    except Exception as e:\n",
    "        log_event(step_name, \"Erro\", message=str(e))  # Registrando erro no log\n",
    "        raise  # Levantando novamente a exceção\n",
    "\n",
    "# Contar os dias distintos representados no arquivo.\n",
    "def distinct_days(logs_df):\n",
    "    \"\"\"\n",
    "    Conta a quantidade de dias distintos representados nos logs.\n",
    "\n",
    "    Parâmetros:\n",
    "    - logs_df (DataFrame): DataFrame contendo os logs com a coluna \"Date\".\n",
    "\n",
    "    Retorna:\n",
    "    - distinct_days_count (int): Quantidade de dias distintos encontrados nos logs.\n",
    "    \"\"\"\n",
    "    step_name = \"Distinct Days\"  # Nome da etapa para rastrear o log de execução\n",
    "    try:\n",
    "        log_event(step_name, \"Inicio\", \"Contando a quantidade de dias representados\")  # Registrando o início da operação\n",
    "        distinct_days_count = logs_df.select(\"Date\").distinct().count()  # Contando os dias distintos\n",
    "        print(f\"Quantidade de dias representados: {distinct_days_count}\")  # Exibindo o resultado\n",
    "        log_event(step_name, \"Sucesso\", record_count=distinct_days_count)  # Registrando o sucesso da operação com a contagem\n",
    "    except Exception as e:\n",
    "        log_event(step_name, \"Erro\", message=str(e))  # Registrando erro no log\n",
    "        raise  # Levantando novamente a exceção\n",
    "\n",
    "# Análise do tamanho das respostas: Calcula total, máximo, mínimo e média dos bytes.\n",
    "def response_size_analysis(logs_df):\n",
    "    \"\"\"\n",
    "    Realiza a análise do tamanho das respostas, calculando o total, máximo, mínimo e médio de bytes.\n",
    "\n",
    "    Parâmetros:\n",
    "    - logs_df (DataFrame): DataFrame contendo os logs com a coluna \"Bytes\" que representa o tamanho das respostas.\n",
    "\n",
    "    Retorna:\n",
    "    - Imprime os resultados no console e registra o evento no log.\n",
    "    \"\"\"\n",
    "    step_name = \"Byte das Respostas\"  # Nome da etapa para rastrear o log de execução\n",
    "    try:\n",
    "        log_event(step_name, \"Inicio\", \"Analisando o tamanho das respostas\")  # Registrando o início da operação\n",
    "        total_volume = logs_df.select(sum(\"Bytes\")).collect()[0][0]  # Calculando o total de bytes\n",
    "        max_volume = logs_df.select(max(\"Bytes\")).collect()[0][0]  # Calculando o valor máximo de bytes\n",
    "        min_volume = logs_df.select(min(\"Bytes\")).collect()[0][0]  # Calculando o valor mínimo de bytes\n",
    "        avg_volume = logs_df.select(avg(\"Bytes\")).collect()[0][0]  # Calculando a média de bytes\n",
    "        # Exibindo os resultados no console\n",
    "        print(f\"Total de dados: {total_volume} bytes\")\n",
    "        print(f\"Maior volume em uma resposta: {max_volume} bytes\")\n",
    "        print(f\"Menor volume em uma resposta: {min_volume} bytes\")\n",
    "        print(f\"Volume médio de dados: {avg_volume:.2f} bytes\")\n",
    "        log_event(step_name, \"Sucesso\", message=f\"Total: {total_volume}, Máximo: {max_volume}, Mínimo: {min_volume}, Médio: {avg_volume:.2f}\")  # Registrando o sucesso da operação\n",
    "    except Exception as e:\n",
    "        log_event(step_name, \"Erro\", message=str(e))  # Registrando erro no log\n",
    "        raise  # Levantando novamente a exceção\n",
    "\n",
    "# Identificar o dia da semana com maior número de erros \"HTTP Client Error\".\n",
    "def client_error_day_of_week(logs_df):\n",
    "    \"\"\"\n",
    "    Identifica o dia da semana com maior número de erros HTTP do tipo \"Client Error\" (Status 400-499).\n",
    "\n",
    "    Parâmetros:\n",
    "    - logs_df (DataFrame): DataFrame contendo os logs com informações de status e dia da semana.\n",
    "\n",
    "    Retorna:\n",
    "    - Exibe os resultados no console e registra o evento no log.\n",
    "    \"\"\"\n",
    "    step_name = \"Dia das Semana com Mais Erros\"  # Nome da etapa para rastrear o log de execução\n",
    "    try:\n",
    "        log_event(step_name, \"Inicio\", \"Identificando o dia da semana com maior número de erros HTTP Client Error\")  # Registrando o início da operação\n",
    "        client_errors = logs_df.filter((col(\"Status\") >= 400) & (col(\"Status\") < 500))  # Filtrando os logs para os erros de cliente (Status 400-499)\n",
    "        errors_by_day = client_errors.groupBy(\"Day_of_Week_Name\").count().orderBy(col(\"count\").desc())  # Agrupando por dia da semana e contando os erros\n",
    "        errors_by_day.show(truncate=False)  # Exibindo os resultados no console\n",
    "        log_event(step_name, \"Sucesso\", record_count=errors_by_day.count())  # Registrando o sucesso da operação\n",
    "    except Exception as e:\n",
    "        log_event(step_name, \"Erro\", message=str(e))  # Registrando erro no log\n",
    "        raise  # Levantando novamente a exceção\n",
    "\n",
    "# Converter logs para DataFrame Spark e salvar em Delta.\n",
    "def log_to_delta(processing_log, log_delta_path):\n",
    "    \"\"\"\n",
    "    Converte o log processado para um DataFrame e salva no formato Delta.\n",
    "\n",
    "    Parâmetros:\n",
    "    - processing_log (list): Lista contendo os logs processados.\n",
    "    - log_delta_path (str): Caminho onde os logs serão salvos no formato Delta.\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame no formato Delta e imprime a confirmação no console.\n",
    "    \"\"\"\n",
    "    log_df = spark.createDataFrame(processing_log)  # Criando o DataFrame a partir da lista de logs\n",
    "    log_df.write.format(\"delta\").mode(\"overwrite\").save(log_delta_path)  # Salvando o DataFrame no Delta\n",
    "    print(f\"Logs salvos no Delta Lake no caminho: {log_delta_path}\")  # Confirmando no console\n",
    "\n",
    "# Consultar dados do Delta Lake com Spark SQL.\n",
    "def query_delta_logs(log_delta_path, query):\n",
    "    \"\"\"\n",
    "    Consulta os dados no Delta Lake usando uma query SQL.\n",
    "\n",
    "    Parâmetros:\n",
    "    - log_delta_path (str): Caminho para os dados armazenados no Delta Lake.\n",
    "    - query (str): Consulta SQL a ser executada nos dados.\n",
    "\n",
    "    Retorna:\n",
    "    - result (DataFrame): DataFrame contendo os resultados da consulta.\n",
    "    \"\"\"\n",
    "    spark.read.format(\"delta\").load(log_delta_path).createOrReplaceTempView(\"logs_temp\")  # Lê os dados do Delta Lake e cria uma view temporária para consulta\n",
    "    result = spark.sql(query)  # Executa a consulta SQL no Delta\n",
    "    result.show()  # Exibe o resultado da consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d429de-f3ee-4915-b3e8-62ca5cc8dbb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingestão e Tratamento por Camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af0ebd7c-465f-49f1-9ee6-fca4e271e737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 0 bytes.\n"
     ]
    }
   ],
   "source": [
    "# Ler arquivo Full Load: Carrega o arquivo de log e retorna um DataFrame.\n",
    "logs_df = read_logs(file_path)\n",
    "\n",
    "# Estruturação e Definição de Schema: Converte dados brutos para formato estruturado.\n",
    "logs_parsed_df = bronze_to_silver(logs_df)\n",
    "\n",
    "# Refina os dados para o formato (gold): Acrescentando colunas com informações adicionais.\n",
    "logs_parsed_df = silver_to_gold(logs_parsed_df)\n",
    "\n",
    "# Cria um Parquet no Blob Storage para ser consumido pelo Power BI.\n",
    "save_to_blob(logs_parsed_df, full_load_path)\n",
    "\n",
    "# Aciona um trigger no Power Automate para que o Power BI seja atualizado.\n",
    "trigger()\n",
    "\n",
    "# Salva o DataFrame processado como uma tabela Delta\n",
    "save_as_delta_table(logs_parsed_df, \"access_log_delta\", delta_table_acessLog_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "593c394d-ed83-4a24-ac68-37b3a815a570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Desafios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca2b136-4008-4ecf-b585-d7cba3d2f3fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n|IP            |count |\n+--------------+------+\n|10.216.113.172|158614|\n|10.220.112.1  |51942 |\n|10.173.141.213|47503 |\n|10.240.144.183|43592 |\n|10.41.69.177  |37554 |\n|10.169.128.121|22516 |\n|10.211.47.159 |20866 |\n|10.96.173.111 |19667 |\n|10.203.77.198 |18878 |\n|10.31.77.18   |18721 |\n+--------------+------+\n\n+----------------+------+\n|Endpoint        |count |\n+----------------+------+\n|release-schedule|122178|\n|home-page       |99303 |\n|search          |24073 |\n|trailers        |18679 |\n|about-us        |16757 |\n|news            |15206 |\n+----------------+------+\n\nQuantidade de IPs distintos: 333923\nQuantidade de dias representados: 790\nTotal de dados: 805219145090 bytes\nMaior volume em uma resposta: 80215074 bytes\nMenor volume em uma resposta: 1 bytes\nVolume médio de dados: 195014.83 bytes\n+----------------+-----+\n|Day_of_Week_Name|count|\n+----------------+-----+\n|Sexta           |14787|\n|Quarta          |12700|\n|Segunda         |12410|\n|Quinta          |11922|\n|Terça           |11650|\n|Sábado          |10898|\n|Domingo         |10652|\n+----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Carregar a Delta Table como DataFrame\n",
    "logs_parsed_df = spark.read.format(\"delta\").load(delta_table_acessLog_path)\n",
    "\n",
    "# Realiza uma análise para contar as 10 maiores origens de acesso (IPs).\n",
    "top_ip_count(logs_parsed_df)\n",
    "\n",
    "# Realiza uma análise para contar os 6 endpoints mais acessados.\n",
    "top_endpoints_count(logs_parsed_df)\n",
    "\n",
    "# Conta a quantidade de IPs distintos presentes nos logs.\n",
    "distinct_ips(logs_parsed_df)\n",
    "\n",
    "# Conta a quantidade de dias distintos representados no arquivo de log.\n",
    "distinct_days(logs_parsed_df)\n",
    "\n",
    "# Realiza uma análise sobre o tamanho das respostas.\n",
    "response_size_analysis(logs_parsed_df)\n",
    "\n",
    "# Identifica o dia da semana com o maior número de erros \"HTTP Client Error\".\n",
    "client_error_day_of_week(logs_parsed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc824a6-3aa7-4aa0-8e36-74c8aaa4d900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Armazenamento dos Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be74d442-7763-4c49-982f-576030fe4846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs salvos no Delta Lake no caminho: wasbs://accesslog@cloudcasefirst.blob.core.windows.net/log_delta\n+---------+------------+------+-------+-----------+\n|Timestamp|PipelineStep|Status|Message|RecordCount|\n+---------+------------+------+-------+-----------+\n+---------+------------+------+-------+-----------+\n\n+--------------------+--------------------+-------+--------------------+-----------+\n|           Timestamp|        PipelineStep| Status|             Message|RecordCount|\n+--------------------+--------------------+-------+--------------------+-----------+\n|2025-01-24 02:11:...|    Read Access Logs|Sucesso|                    |    4477843|\n|2025-01-24 02:11:...|    Bronze to Silver|Sucesso|                    |       null|\n|2025-01-24 02:11:...|      Silver to Gold|Sucesso|                    |       null|\n|2025-01-24 02:16:...|        Save to Blob|Sucesso|                    |       null|\n|2025-01-24 02:16:...|             Trigger|Sucesso|                    |       null|\n|2025-01-24 02:18:...|  Full Load to Delta|Sucesso|                    |    4477843|\n|2025-01-24 02:19:...|   Top 10 - IP Count|Sucesso|                    |         10|\n|2025-01-24 02:19:...|Top 6 - Endpoints...|Sucesso|                    |          6|\n|2025-01-24 02:19:...|        Distinct IPs|Sucesso|                    |     333923|\n|2025-01-24 02:19:...|       Distinct Days|Sucesso|                    |        790|\n|2025-01-24 02:20:...|  Byte das Respostas|Sucesso|Total: 8052191450...|       null|\n|2025-01-24 02:20:...|Dia das Semana co...|Sucesso|                    |          7|\n+--------------------+--------------------+-------+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Converte a lista 'processing_log' em um DataFrame do Spark e salva no Delta Lake.\n",
    "log_to_delta(processing_log, log_delta_path)\n",
    "\n",
    "# Executa e exibe uma consulta SQL no Spark para filtrar os registros da tabela Delta onde o status é 'Erro'.\n",
    "query = \"SELECT * FROM logs_temp WHERE Status = 'Erro' ORDER BY Timestamp ASC\"\n",
    "query_delta_logs(log_delta_path, query)\n",
    "\n",
    "# Executa e exibe uma consulta SQL no Spark para filtrar os registros da tabela Delta onde o status é 'Sucesso'.\n",
    "query = \"SELECT * FROM logs_temp WHERE Status = 'Sucesso' ORDER BY Timestamp ASC\"\n",
    "query_delta_logs(log_delta_path, query)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "web_log_analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
